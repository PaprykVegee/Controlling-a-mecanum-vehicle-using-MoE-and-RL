{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c388f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchvision import transforms\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor \n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92b007f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(img: np.array):\n",
    "  pil_img = Image.fromarray(img)\n",
    "  pil_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da0d908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.load(r\"/home/developer/ros2_ws/src/method_tester/concat_lidar_mask.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7795498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAACnklEQVR4nO3dTXKbMBiA4Y8ZHaWH6WGy6FFYcJgcpYseRYsu7E5j4j+BsBF6nk0MCw/2jN6IBMQQR5NTRJ7vTDlSTjnlf1tx2o48vPr4Nnb+6OdPl3LKkSIiR8oRKaeInCJHOu+J9M6DhR04VAO+tQ945DgNEABY4igNUABY5ggNMP5huQM0QAJghdYbIACwTtMNEABYreEGKABU0GoDBADqaLEBxj/U014DFABqaq0BCgB1NdYACYDKNAD6pgHQNw2AvmkA9K2tBkgA1KYB0DcNgL612oB0mYPUXR3y5eucT3vufQ959qK77wyuaTWCQB0aAH3TAOibBkDfNAD6pgHQt7Ya8Pti68ebjgKOpOUGzGkClDtSA2YkAZ5w4AbMaQJc0VEDZiQBInpuABDRWgPGiJ/vPgY4luHZX62fXzd+fWxwKHdN49ctIYBaHjXgc7b9Z7MjeWx2Bi8EUMGNBsyHfoWxP0XE6unD5XQghADW+t+Ab8M+as75p1rv9K0COgBrDONsx+tP9UtdqYAOwFLDuN2gnypM/q+/7XhtvwzAAsMmBZgithn/57cfr+9XAaDUNhHc2q2LHFUQSlVowHT68bqY3JoHhQhAsRbnAfdudRABKNNeA+7MAkIDoNRzDXj5dP+2R/c7igAUud+A09jfw9A/e3jHswRAmZbOBZ5Z8kADoMzwMY0tXB355KInEgCFGpkHPLnqkQRAqSYa8Oy6Z78stASl9t+AgoUPTQOg2L4bcP9agDkJgHI7bkDpyscSAAvstQHFS58rACyyxwaUnQFEhALAYrtrwJJnnygALLarBix79pH/CMIK+2nA0oefmQTAGvtowIK/AJwpAKzz9gYsH/6hALDeWxuw8tnHCgDrva0B6599XicBfwGPQXzCY/EpsQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256 at 0x70A67C26A5F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256 at 0x70A67C26B6A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(sample[:, :, 0])\n",
    "plot(sample[:, :, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceaa2957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smp.Unet(\n",
    "\t\tencoder_name=\"resnet18\",\n",
    "\t\tencoder_weights=\"imagenet\",\n",
    "\t\tin_channels=3,\n",
    "\t\tclasses=8\n",
    ")\n",
    "checkpoint = torch.load(r\"/home/developer/ros2_ws/src/UNET_trening/best-unet-epoch=05-val_dice=0.9838.ckpt\", map_location=DEVICE)\n",
    "state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "\t\tif k.startswith('model.'):\n",
    "\t\t\t\tnew_state_dict[k[len('model.'):]] = v\n",
    "\t\telse:\n",
    "\t\t\t\tnew_state_dict[k] = v\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "877df586",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.encoder\n",
    "conv1 = encoder.conv1\n",
    "encoder.conv1 = nn.Conv2d(in_channels=2, out_channels=conv1.out_channels, kernel_size=conv1.kernel_size, stride=conv1.stride, padding=conv1.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ab61fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, model):\n",
    "        super().__init__(observation_space, features_dim=1)\n",
    "\n",
    "        self.encoder = model.encoder\n",
    "        conv1 = self.encoder.conv1\n",
    "\n",
    "        self.encoder.conv1 = nn.Conv2d(\n",
    "            in_channels=2,\n",
    "            out_channels=conv1.out_channels,\n",
    "            kernel_size=conv1.kernel_size,\n",
    "            stride=conv1.stride,\n",
    "            padding=conv1.padding\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # policzenie features_dim\n",
    "        with torch.no_grad():\n",
    "            sample = torch.zeros(1, *observation_space.shape)\n",
    "            feats = self.encoder(sample)\n",
    "\n",
    "            if isinstance(feats, list):\n",
    "                feats = feats[-1]   # ostatni feature map\n",
    "\n",
    "            feats = self.pool(feats)\n",
    "            self._features_dim = feats.view(1, -1).shape[1]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        feats = self.encoder(x)\n",
    "\n",
    "        if isinstance(feats, list):\n",
    "            feats = feats[-1]\n",
    "\n",
    "        feats = self.pool(feats)\n",
    "        return feats.flatten(start_dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a61a2958-d15c-497a-9a7e-bd81a9cc81d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "observation_space = gym.spaces.Box(\n",
    "    low=0,\n",
    "    high=255,\n",
    "    shape=(2, 256, 256),\n",
    "    dtype=np.uint8\n",
    ")\n",
    "\n",
    "extractor = AgentFeatureExtractor(observation_space, model)\n",
    "\n",
    "obs = torch.tensor(sample.transpose(2, 1, 0), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "out = extractor(obs)\n",
    "\n",
    "print(out.shape)\n",
    "print(extractor.features_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
